```
import requests
from bs4 import BeautifulSoup
import os
from hashlib import md5


def get_html(url, headers):
	html = requests.get(url, headers)
	return html.text


def parse_html(html):
	'''
	解析贴吧首页的帖子href放入列表
	:param html:
	:return:
	'''
	html_soup = BeautifulSoup(html, 'lxml')
	# 拿到每个帖子的超级连接放到列表
	a_list = html_soup.select('.j_thread_list .threadlist_title a')
	# 再从每个a里面拿到每个帖子的href连接地址，形成列表
	href_list = [a.get('href') for a in a_list]
	return href_list


def parse_image(img_html):
	'''
	解析每个帖子里面的图片src放入列表
	:param img_html:
	:return:
	'''
	img_html_soup = BeautifulSoup(img_html, 'lxml')
	img_list = img_html_soup.select('.BDE_Image')
	src_list = [img.get('src') for img in img_list]
	return src_list


def download_img(src, headers):
	# 如果没有文件夹创建一个
	dirname = 'zhaoliying'
	if not os.path.exists(dirname):
		os.mkdir(dirname)
	# 请求下载图片内容
	Imgcontent = requests.get(src, headers).content

	# 避免图片重复写文件
	file = md5(str(Imgcontent).encode('utf-8')).hexdigest()
	filename = dirname + '\\' + file + '.jpg'
	if not os.path.exists(filename):
		with open(filename, 'wb') as f:
			f.write(Imgcontent)


def main():
	# 第一步先拿到颖宝吧的首页代码
	url = 'http://tieba.baidu.com/f?kw=%E8%B5%B5%E4%B8%BD%E9%A2%96&pn=0'
	headers = {
		'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'
	}
	html = get_html(url, headers)
	# print(html)

	# 第二步拿到当前页里面每个帖子的连接href属性
	href_list = parse_html(html)
	print(href_list)

	# 第三步在循环遍历这个href列表去到每个帖子里面拿图片的src
	for href in href_list:
		# 遍历拿到href需要拼接为一个完整的url，继续调用获取html
		url = 'http://tieba.baidu.com' + href
		img_html = get_html(url, headers)
		src_list = parse_image(img_html)

		# 最后遍历这个src把每个src图片下载
		for src in src_list:
			download_img(src, headers)


if __name__ == '__main__':
	main()
```

```
import requests
from lxml import etree


# 写入文件
def write_file(art):
	with open("笑话.txt", "a+", encoding="utf-8") as f:
		f.write(art)


# 解析html得到自己想要的内容
def parse_html(html):
	content = etree.HTML(html)
	a_lists = content.xpath('//div[@class="list_title"]/ul/li/b/a/@href')
	for a in a_lists:
		# # "http://www.jokeji.cn/jokehtml/%E5%86%B7%E7%AC%91%E8%AF%9D/201806212319307.htm"
		url = "http://www.jokeji.cn" + a
		result = requests.get(url)
		# 转化成gb2312编码
		result.encoding = "gb2312"
		result = result.text
		info = etree.HTML(result)
		art_lists = info.xpath('//span[@id="text110"]/p/text()')
		for art in art_lists:
			print(art)
			write_file(art)


def main():
	num = 1
	for i in range(10):
		url = "http://www.jokeji.cn/list_" + str(num) + ".htm"
		num += 1
		headers = {
			"User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"
		}
		html = requests.get(url, headers=headers)
		# 如果不知道是什么编码 此时可以print(html.encoding)查看一下是什么编码
		html.encoding = "gb2312"
		html = html.text
		parse_html(html)


if __name__ == '__main__':
	main()
```
